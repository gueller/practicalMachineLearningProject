---
title: "Exercise Method Prediction"
author: "Gueller"
date: "7/18/2020"
output:
  html_document: default
  pdf_document: default
---
### Overview

In this assignment, we will use data collected as part of a Human Activity Recognition (HAR) study and machine learning to determine how effective the data collected is for prediction of the quality of the exercise activity.

The activity is "Unilaterial Dumbbell Biceps Curl".  Each participant was asked to perform 10 repetitions, 5 different methods.  Essentially, one method was correct, the other four were incorrect in a prescribed manner.  The manner in which the activity was completed is captured in the "classe" field; the meaning of the categorizations is shown in the appendix.

### Executive Summary

### Setting the Environment
```{r setup, echo=TRUE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo=FALSE)
# Always set the seed
set.seed(1994)
# Libraries
library(caret)
library(rpart)
library(rattle)
library(randomForest)
library(knitr)
library(corrplot)
library(RColorBrewer)
library(kableExtra)
```

### Loading, Cleaning and Arranging the Data
```{r gettingTheData, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
# locations of the data
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

# read the data in
training <- read.csv(url(trainURL))
testing <- read.csv(url(testURL))
dim(testing)
```
We will be working with the training set for now, holding out the testing set for the quiz.

Exploration of the data set using head (not shown here) revealed that the first  7 columns of the data are identifiers that will be removed. Using the is.na function revealed that a number of the fields were almost entirely NA, and those will be removed as well. 

```{r removeIdentAndNA, echo=TRUE, message=FALSE, warning=FALSE}
# Remove Identifiers
training <- training[,-(1:7)]
dim(training)
# Remvove NA's
remNA <- sapply(training, function(x) mean(is.na(x))) > 0.975
training <- training[,remNA==FALSE]
dim(training)
```

The near zero variance variables will be identified and removed.

```{r nearZeroVariance, echo=TRUE, message=FALSE, warning=FALSE}
nearZero <- nearZeroVar(training)
# nearZero
training <- training[,-nearZero]
dim(training)
```

The testing set will be broken into trainSet and testSet.

```{r partionTraining, echo=TRUE, message=FALSE, warning=FALSE}
# Break the training set into training and test
inTrain <-createDataPartition(training$classe, p=0.7, list=FALSE)
trainSet <- training[inTrain,]
testSet <- training[-inTrain,]
dim(trainSet);dim(testSet)
# set classe as.factor
trainSet$classe <- as.factor(trainSet$classe)
testSet$classe <- as.factor(testSet$classe)
levels(trainSet$classe);levels(testSet$classe)
```

### Correlation Analysis

We will look at the number of highly correlated variables to see if a Principal Component Analysis should be done.

```{r correlationMatrix, echo=TRUE, message=FALSE, warning=FALSE, fig.width=6, fig.height=8, fig.align='left'}
correlationMatrix <- cor(trainSet[,-53])
corrplot(correlationMatrix, order="FPC", method="color", type="lower",
    tl.cex=0.6, tl.col=rgb(0,0,0))
```

From the graph, there appears to be a number of $^+_-$highly correlated variables.  If PCA is performed, interpretability will be lost and will not be performed.  PCA could be explored if scalability of the select model becomes necessary.

### Modeling

Three models will be build with the trainSet data; Decision Tree, Random Forest and Generalized Boosted.  The models will be build and cross-validated with the testSet.  The model that has the highest accuracy will be used against the quiz 

## Decision Tree

```{r modelDecisionTree, echo=TRUE, message=FALSE, warning=FALSE, fig.width=10, fig.height=8, fig.align='left', cache=TRUE}
mdl_dt <- rpart(classe~., data=trainSet, method="class")
fancyRpartPlot(mdl_dt, main="Decision Tree", sub="", palettes=c("Reds", "Greys"))
```

```{r predictDecisionTree, echo=TRUE, message=FALSE, warning=FALSE}
prd_dt <- predict(mdl_dt, newdata=testSet, type="class")
cm_dt <- confusionMatrix(prd_dt, testSet$classe)
acc_dt <- round(cm_dt$overall['Accuracy'],4)
kap_dt <- round(cm_dt$overall['Kappa'],4)
```

## Random Forest

```{r modelRandomForest, echo=TRUE, message=TRUE, warning=TRUE, cache=TRUE}
set.seed(1994)
ctrl_rf <- trainControl(method="cv", number=3, verboseIter=FALSE)
mdl_rf <- train(classe~., data=trainSet, method="rf",
    trControl=ctrl_rf)
mdl_rf$finalModel
```

```{r predictRandomForest, echo=TRUE, message=FALSE, warning=FALSE}
prd_rf <- predict(mdl_rf, newdata=testSet)
cm_rf <- confusionMatrix(prd_rf, testSet$classe)
cm_rf
acc_rf <- round(cm_rf$overall['Accuracy'],4)
kap_rf <- round(cm_rf$overall['Kappa'],4)
```

## Generalized Boosted Method

```{r modelGeneralizedBoostedMethod, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
set.seed(1994)
ctrl_gbm <- trainControl(method="repeatedcv", number=5, repeats=1)
mdl_gbm <- train(classe~., data=trainSet, method="gbm",
    trControl=ctrl_gbm, verbose=FALSE)
mdl_gbm$finalModel
```

```{r predictGeneralizedBoostedMethod, echo=TRUE, message=FALSE, warning=FALSE}
prd_gbm <- predict(mdl_gbm, newdata=testSet)
cm_gbm <- confusionMatrix(prd_gbm, testSet$classe)
cm_gbm
acc_gbm <- round(cm_gbm$overall['Accuracy'],4)
kap_gbm <- round(cm_gbm$overall['Kappa'],4)
```

### Results
```{r results, echo=TRUE, warning=FALSE, message=FALSE}
result_dt <- as.data.frame(rbind(c(acc_dt,kap_dt)))
result_rf <- as.data.frame(rbind(c(acc_rf,kap_rf)))
result_gbm <- as.data.frame(rbind(c(acc_gbm,kap_gbm)))
tab <- rbind(result_dt, result_rf, result_gbm)
row.names(tab) <- c("Decision Tree", "Random Forest", "Generalized Boosted Method")
# Pretty output
kable(tab, "latex", caption="Results") %>%
    kable_styling(full_width=FALSE, position="left", 
        latex_options="striped", stripe_index=c(2),
        stripe_color = "yellow")
```






### We Break for Nobody

### Appendix
Overview Information:

- http://groupware.les.inf.puc-rio.br/har

- Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Dataset provided by Instructor taken from:

- http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

Visualization

- The Classe decision tree was plotted using the R Rattle package.



"classe"

Class A - Exactly according to the specification

Class B - Throwing the elbows to the front

Class C - Lifting the dumbbell only halfway

Class D - Lowering the dumbbell only halfway
Class E - Throwing the hips to the front